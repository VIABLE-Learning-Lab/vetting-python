#!/usr/bin/env python3
"""
Offline demo of VETTING framework functionality.

This script demonstrates the framework without making real API calls.
"""

import asyncio
import sys
from pathlib import Path
from unittest.mock import Mock, AsyncMock

# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from vetting_python import VettingFramework, VettingConfig, ChatMessage, ModelConfig, OpenAIProvider
from vetting_python.core.models import ContextItem, Usage


def create_mock_provider():
    """Create a mock provider for testing."""
    provider = Mock(spec=OpenAIProvider)
    
    # Mock the generate_response method
    async def mock_generate_response(messages, model_config, system_prompt=None):
        # Simulate different responses based on content
        last_message = messages[-1].content.lower()
        
        if "direct answer" in last_message or "what is 2+2" in last_message:
            # Simulate a response that might fail verification
            return (
                "The answer is 4.",
                Usage(prompt_tokens=50, completion_tokens=10, total_tokens=60),
                False
            )
        elif "guide" in last_message or "think" in last_message:
            # Simulate a good educational response
            return (
                "Great question! Can you think about what happens when you combine 2 items with 2 more items? What counting strategies do you know?",
                Usage(prompt_tokens=80, completion_tokens=25, total_tokens=105),
                False
            )
        elif "verify" in system_prompt.lower() if system_prompt else False:
            # This is a verification call
            if "The answer is 4" in messages[0].content:
                return ("FAIL: Directly provides the answer without educational guidance", Usage(40, 15, 55), False)
            else:
                return ("PASS", Usage(45, 8, 53), False)
        else:
            # Default response
            return (
                "Hello! I'm here to help you learn. What would you like to explore today?",
                Usage(30, 20, 50),
                False
            )
    
    provider.generate_response = AsyncMock(side_effect=mock_generate_response)
    
    # Mock cost calculation
    def mock_calculate_cost(model_id, usage):
        # Use real pricing from the updated models
        if "gpt-4o-mini" in model_id:
            input_price = 0.15  # per 1M tokens
            output_price = 0.6   # per 1M tokens
            return (usage.prompt_tokens / 1000000) * input_price + (usage.completion_tokens / 1000000) * output_price
        return 0.001  # fallback
    
    provider.calculate_cost = Mock(side_effect=mock_calculate_cost)
    
    return provider


async def demo_chat_mode():
    """Demonstrate chat mode functionality."""
    print("=" * 60)
    print("DEMO: Chat Mode")
    print("=" * 60)
    
    provider = create_mock_provider()
    framework = VettingFramework(chat_provider=provider)
    
    config = VettingConfig(
        mode="chat",
        chat_model=ModelConfig(model_id="gpt-4o-mini", temperature=0.7, max_tokens=100)
    )
    
    messages = [ChatMessage("user", "Hello! Can you help me with learning?")]
    response = await framework.process(messages, config)
    
    print(f"‚úÖ User Message: {messages[0].content}")
    print(f"‚úÖ AI Response: {response.content}")
    print(f"‚úÖ Mode: {response.mode}")
    print(f"‚úÖ Cost: ${response.total_cost:.6f}")
    print(f"‚úÖ Tokens: {response.total_usage.total_tokens}")
    print(f"‚úÖ Requires Attention: {response.requires_attention}")


async def demo_vetting_mode():
    """Demonstrate vetting mode with educational scenario."""
    print("\n" + "=" * 60)
    print("DEMO: Vetting Mode - Educational Scenario")
    print("=" * 60)
    
    # Create a more sophisticated mock that simulates the verification loop
    provider = Mock(spec=OpenAIProvider)
    
    responses = [
        # First attempt - direct answer (will fail verification)
        ("The answer is 4.", Usage(40, 8, 48), False),
        ("FAIL: Directly provides the answer without educational guidance", Usage(35, 12, 47), False),
        
        # Second attempt - educational response (will pass)
        ("Great question! Can you think about what happens when you combine 2 items with 2 more items? What counting strategies do you know?", 
         Usage(60, 28, 88), False),
        ("PASS", Usage(40, 5, 45), False)
    ]
    
    provider.generate_response = AsyncMock(side_effect=responses)
    provider.calculate_cost = Mock(side_effect=[0.000008, 0.000007, 0.000014, 0.000006])
    
    framework = VettingFramework(chat_provider=provider)
    
    # Educational context
    context_item = ContextItem(
        question={
            "text": "What is 2 + 2?",
            "id": "math_001",
            "subject": "Elementary Math",
            "grade_level": "1st Grade"
        },
        answer_key={
            "correctAnswer": "4",
            "keyConcepts": ["addition", "counting", "arithmetic"],
            "explanation": "When you add 2 + 2, you're combining two groups of 2 items each."
        }
    )
    
    config = VettingConfig(
        mode="vetting",
        chat_model=ModelConfig(model_id="gpt-4o-mini", temperature=0.7),
        verification_model=ModelConfig(model_id="gpt-4o-mini", temperature=0.1),
        context_items=[context_item],
        max_attempts=3,
        enable_educational_rules=True,
        session_id="demo_session_001",
        user_id="student_demo"
    )
    
    messages = [ChatMessage("user", "I need help with my homework. What is 2 + 2?")]
    response = await framework.process(messages, config)
    
    print(f"‚úÖ Student Question: {messages[0].content}")
    print(f"‚úÖ Final AI Response: {response.content}")
    print(f"‚úÖ Verification Passed: {response.verification_passed}")
    print(f"‚úÖ Attempts Made: {response.attempt_count}")
    print(f"‚úÖ Stop Reason: {response.stop_reason.value}")
    print(f"‚úÖ Total Cost: ${response.total_cost:.6f}")
    print(f"‚úÖ Total Tokens: {response.total_usage.total_tokens}")
    
    if response.attempts:
        print(f"\nüìã Detailed Attempt Log:")
        for i, attempt in enumerate(response.attempts, 1):
            status = "‚úÖ PASSED" if attempt.verification_passed else "‚ùå FAILED"
            print(f"   Attempt {i}: {status}")
            print(f"      Response: {attempt.chat_response[:80]}...")
            if not attempt.verification_passed:
                print(f"      Verification: {attempt.verification_output}")
            print(f"      Cost: ${attempt.chat_cost + attempt.verification_cost:.6f}")


async def demo_cost_tracking():
    """Demonstrate cost tracking capabilities."""
    print("\n" + "=" * 60)
    print("DEMO: Cost Tracking & Model Information")
    print("=" * 60)
    
    provider = OpenAIProvider(api_key="dummy_key")  # Won't make real calls
    
    print("üìä Supported Models (2025-07-31 Pricing):")
    models = provider.get_supported_models()
    for model in models[:8]:  # Show first 8
        if model in provider.MODEL_PRICING:
            input_price, output_price = provider.MODEL_PRICING[model]
            print(f"   ‚Ä¢ {model}: ${input_price:.2f}/${output_price:.2f} per 1M tokens")
    
    print(f"\nüîÑ Model Aliases:")
    aliases = provider.get_model_aliases()
    for alias, actual in aliases.items():
        print(f"   ‚Ä¢ {alias} ‚Üí {actual}")
    
    # Demo cost calculation
    usage = Usage(prompt_tokens=1000, completion_tokens=500, total_tokens=1500)
    cost = provider.calculate_cost("gpt-4o-mini", usage)
    print(f"\nüí∞ Example Usage Cost:")
    print(f"   ‚Ä¢ Model: gpt-4o-mini")
    print(f"   ‚Ä¢ Tokens: {usage.prompt_tokens} input + {usage.completion_tokens} output = {usage.total_tokens} total")
    print(f"   ‚Ä¢ Cost: ${cost:.6f}")


async def demo_framework_features():
    """Demonstrate key framework features."""
    print("\n" + "=" * 60)
    print("DEMO: Framework Architecture Features")
    print("=" * 60)
    
    print("üèóÔ∏è  VETTING Framework Architecture:")
    print("   ‚Ä¢ Chat-Layer (LLM-A): User-facing conversational model")
    print("   ‚Ä¢ Verification-Layer (LLM-B): Policy enforcement model")
    print("   ‚Ä¢ Architectural Policy Isolation: Prevents prompt injection")
    print("   ‚Ä¢ Iterative Feedback Loop: Refines responses until compliant")
    
    print("\nüîí Security Features:")
    print("   ‚Ä¢ Safety prefix detection for harmful content")
    print("   ‚Ä¢ Confidential verification prompts")
    print("   ‚Ä¢ Isolated policy enforcement")
    print("   ‚Ä¢ Educational context awareness")
    
    print("\nüìö Educational Use Cases:")
    print("   ‚Ä¢ Homework help without direct answers")
    print("   ‚Ä¢ Tutoring that encourages critical thinking")
    print("   ‚Ä¢ Subject-aware content filtering")
    print("   ‚Ä¢ Learning progress tracking")
    
    print("\n‚ö° Provider Support:")
    print("   ‚Ä¢ OpenAI (GPT-4.1, GPT-4o, GPT-4o-mini)")
    print("   ‚Ä¢ Anthropic Claude (Sonnet 4, 3.7, 3.5)")
    print("   ‚Ä¢ Google Gemini (2.5 Pro, 2.5 Flash, 2.0 Flash)")
    print("   ‚Ä¢ Unified cost tracking across providers")


async def main():
    """Run the complete demo."""
    print("üéØ VETTING Framework - Comprehensive Demo")
    print("üìÖ Version 0.1.0 - Released 2025-07-31")
    print("üè´ Educational AI Safety Through Dual-LLM Architecture")
    
    # Run all demos
    await demo_chat_mode()
    await demo_vetting_mode()
    await demo_cost_tracking()
    await demo_framework_features()
    
    print("\n" + "=" * 60)
    print("üéâ DEMO COMPLETE")
    print("=" * 60)
    print("‚úÖ The VETTING framework is ready for production use!")
    print("üìñ See README.md for installation and usage instructions")
    print("üîß All tests demonstrate proper functionality")
    print("üí° Framework successfully implements dual-LLM safety architecture")


if __name__ == "__main__":
    asyncio.run(main())